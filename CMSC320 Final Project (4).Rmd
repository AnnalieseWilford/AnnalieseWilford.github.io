---
title: "CMSC 320 Final Project"
author: "Samarth Lakhotia, Xavier Warmerdam, Annaliese Wilford"
date: "5/18/2020"
output: html_document
---
## Introduction 
This tutorial notebook will be outlining the whole data science pipeline: From data curation to management, along with performing an exploratory data analysis that will return various findings about the data and help us decide making relevant predictions using machine learning.

We will divide this entire pipeline into steps and explain how each step leads to the next.

## Dataset Choice and Motivation
To demonstrate several data science concepts, we will be using the dataset provided by Airbnb for Denver, Colorado for the year 2019. This dataset can be found at it's original source, Inside Airbnb, [here](http://insideairbnb.com/get-the-data.html) We will aim to create a model trained on our dataset which will predict the price of any given Airbnb listing.

Airbnb has increasingly become popular for its online homestaying and lodging services. It connects various "hosts" or renters to travellers, tourists and soon-to-be-locals looking for temporary stay.

The data revolving around this industry can be important in various aspects of the tourism industry, the local and state government as well as prospective hosts and users of this platform. For example, the tourism industry can benefit from the data about the number of tourists visiting the country and the busiest areas of the country. The local and state government can further analyse insights about areas in the state, number of airbnb hosts, the number of users and bookings, etc to develop policies and initiatives to further the value, security and events in these areas. In addition, being able to effectively predict price gives the opportunity for Airbnb to add a feature that would suggest to a host the price at which they should this the property, similar to Zillow's home price estimate feature, Zestimate.

In the upcoming sections, we will describe the specifics of the dataset.

## Table of Contents {#index}

The tutorial will contain the following index:

1. [Data Curation, Parsing and Data Management](#dc_p)
    i. [Setting up R libraries](#loading_libraries)
    ii. [Obtaining Data](#obtaining_data)
    iii. [Understanding and Managing Data](#data_management)
          a. [Pruning Data](#prune_data)
          b. [Tidying Data](#tidy_data)
          c. [Analysing Attributes](#attr_analysis)
2. [Exploratory Data Analysis](#eda)
    i. [Categorical Attributes Versus Price](#c_vs_p)
    ii. [Numerical Attributes Versus Price](#n_vs_p)
3. [Machine Learning](#machine_learning)
    i. [Creating Our Model](#c_m)
    ii. [Alternative Models](#a_m)
          a. [Simple Linear Regression Model](#slrm)
          b. [Filtered Linear Regression Model](#flrm)
    iii. [Testing Function for Different Models](#tfdm)
    iv. [Calculating Error for Each Model](#ceem)
4. [Final Analysis](#fa)


## Data Curation, Parsing and Data Management {#dc_p} 

### Loading Libraries {#loading_libraries}
[(Back to index)](#index)

We load the libraries that we would use throughout the tutorial here. 
```{r setupdb, message=FALSE, warning=FALSE}
library(tidyverse)
library(lubridate)
library(readr)
library(broom)
library(modelr)
```

### Scraping Data {#obtaining_data}
[(Back to index)](#index)
The first step is to obtain the data from a data source. Here, our dataset is in the form of a csv file. Using R functions, we need to parse the csv into a R dataframe. Here is how it is done:

```{r data_scrape, include=TRUE, message = FALSE}
df <- read_csv("Denver_airbnb2.csv")
```

### Data Management {#data_management}
[(Back to index)](#index)

#### Pruning Data {#prune_data}
[(Back to index)](#index)

Sometimes datasets have many columns that one won't need to use for their purpose. 
In our dataset, there are many columns which will not be relevant to our model that we will be analysing later. As a result, we select the attributes that we plan to use and discuss in the tutorial. By doing so, we save a lot of computation time as well as have a tidier dataset. 

```{r data_prune, include=TRUE, message = FALSE}
df <- df %>% select(id, host_id, host_is_superhost, neighbourhood,  room_type, accommodates, bathrooms, bedrooms, beds, bed_type, availability_60, number_of_reviews_ltm, review_scores_rating, calculated_host_listings_count, price) #use select to have a data frame with columns that are relevant to our tutorial
df %>%
  head()
```
#### Tidying Data {#tidy_data}
[(Back to index)](#index)

The next step in the data pipeline is to tidy the dataset. By tidying, we refer to resolving empty rows, converting data types of attributes to the required types, renaming attributes with vague names, combining (joining) various sources of data. The end goal is to have a structured data frame. After this step the data is ready to be manipulated and used in analysis.

One example of cleaning that we are going to perform in our airbnb data set is the following:

Currently price is being stored as a string so we convert it to a double.

```{r data_tidy, include=TRUE, message = FALSE}
df$price = str_replace_all(df$price, "[$,]", "" ) # remove non-numeric characters
df <- df %>% # convert values to doubles
  mutate(price = as.double(price))

```

#### Analysing Attributes {#attr_analysis}
[(Back to index)](#index)

To begin with data analysis we need to understand the data set. For each entity in our dataset there are 17 attributes. First we give brief description of each below.

| Name | Description |
|------|-------------|
|id | a unique number given for each listing|
|host_id | unique id given for each host|
|host_is_superhost | a boolean value to determine if the host is an Airbnb superhost|
|neighbourhood | the neighbourhood in Denver which the listing is in|
|room_type  | details whether the list is for a private room, and entire home/apartment or if it is for a shared room|
|accommodates | number of people that the listing can hold|
|bathrooms | number of bathrooms|
|bedrooms | number of bedrooms|
|beds | number of beds|
|bed_type | type of bed (i.e. real bed, futon, etc.)|
|availability_60 | number of days per year that the listing is available to be booked in the next two months|
|number_of_reviews_ltm | number of reviews for this listing on Airbnb in the last twelve months|
|review_scores_rating | rating of Airbnb listing|
|calculated_host_listings_count | number of other listings that the host has on Airbnb|
|price | price in dollars per night|

**Classifying the Attributes**

We want to classify each attribute as either a categorical or numerical attribute. In this way we can keep track of certain conversions to make to analyse relations among variables. To read more on the classification of categorical vs. numerical variables click [here](https://www.hcbravo.org/IntroDataSci/bookdown-notes/measurements-and-data-types.html).

**Categorical Attributes**
The attributes that we would consider categorical are: id, host_id, host_is_superhost, neighbourhood, room_type, bed_type.

**Numeric Attributes**
The attributes that we would consider numeric are: accommodates, bathrooms, bedrooms, beds, availability_60, number_of_reviews_ltm, review_scores_rating, calculated_host_listings_count, price

## Exploratory Data Analysis: {#eda}
[(Back to index)](#index)

Now that we have a tidy dataset, the next step in the pipeline is to "explore" several relations among attributes and visualize them. EDA shows the distribution of data in the dataset, helps us spot problems that skew data and affect data negatively. We coin this process as Statistical Modelling. To read more on Exploratory Data Analysis click [here](https://www.itl.nist.gov/div898/handbook/eda/section1/eda11.htm).

With the airbnb dataset, we will mainly place our focus on the price and see its relation with various other attributes. 

### Initial Expectactions
Based on intuition there are some relationships we expect to see between our attributes and price. We expect luxurious features, such as real bed, private home, higher number of beds, bedrooms, bathrooms, accommodates, to increase the price. We also expect that high ratings will correlate to a higher price. In addition, we also expect that host with many listings will cost more as the listings will likely be run more like a business and therefore the hosts will charge more. 

#### Categorical Attributes vs. Price {#c_vs_p}
[(Back to index)](#index)

We first delve into checking several categorical attributes' effect on price of a listing. Before starting to plot variables, it is a good practice to see the values in the specified attributes in an outline. Hence, we use summary() function to see how price of a listing various in the given dataset. Along with it, we also plot a histogram of price values to see how data is distributed.

```{r check_price, message = FALSE}
df %>% 
  select(price) %>% 
  summary()

df %>% ggplot(aes(x=price)) + geom_histogram()
```

Here we notice that the price of a listing has an unusual large maximum. \$10000 per day for a listing seems unusual and hence has a change of skewing data a lot. In fact, seeing the thrid quartile of the data, we see that we can safely limit the amount of a listing to a maximum of \$250. Therefore, we filter out all the listings that have a price greater than \$250 

```{r check_price_again, message = FALSE}

df <- df %>%
  filter(price <= 250) #only keep entries with price less than or equal to 250 dollars a night

df %>% select(price) %>% summary()
df %>% ggplot(aes(x=price)) + geom_histogram()
```

As you can see, now the price distribution seems plausible.

##### Neighbourhood vs. Price
Now, let us visualize how the price of a listing varies by neighborhood. This gives an insight on the variation among neighborhoods.

We begin by plotting the average price for each neighbourhood.

```{r neighbourhoodVprice, include=TRUE}
neighVprice <- df %>% # calculating the mean of price for each neighborhood
    group_by(neighbourhood) %>%
    summarize(price_avg = mean(price))
neighVprice %>%  #plotting result
    ggplot(aes(x = neighbourhood, y = price_avg)) + 
  geom_point()  +
  theme(axis.text.x = element_text(angle = 90, hjust = 1))
```  

Description:
In our plot we see that there is a significant variance in average price for each neighbourhood. This suggests that there is a statistically significant relation between the neighbourhood of the listing and it's price.


Now let's explore and visualize the relation between the room type and the listing price.

##### Room Type vs. Price
```{r roomTypeVpriceTable, include=TRUE}
roomTypeVprice <- df %>% # calculating mean of price for each room type 
    group_by(room_type) %>%
    summarize(price_avg = mean(price))

roomTypeVprice[with(roomTypeVprice, order(price_avg)), ] # order by mean price

# plot result
boxplot(price~room_type,data = df) 
```  

Description:
As expected, the average price is higher based on the privacy of the room type of the listing.

##### Bed Type vs. Price

It may seem trivial to conclude that the bed type in a listing affects the price of the listing. Let's see what the summary and plot looks like.

```{r bedTypeVpriceTable, include=TRUE}
roomTypeVprice <- df %>% # calculate mean price for each bed type
    group_by(bed_type) %>%
    summarize(price_avg = mean(price))
roomTypeVprice[with(roomTypeVprice, order(price_avg)), ] # order by mean price

#plot result
boxplot(price~bed_type, data = df) 
```   

Description:

Also as expected, closer to a real bed with a regular mattress increases price.

In conclusion for categorical variables, you can see how we can summarize and visualize the plots. Performing such analysis gives us several ideas to further our analysis and choose variables that are important.

#### Numerical Attributes vs. Price {#n_vs_p}
[(Back to index)](#index)

Now let us move forward with analysing relationship between price and other numerical attributes.


##### Accommodates vs. Price
Generally, one can expect that the price of a listing directly depends on the number of people it can hold. Let us confirm this with the data we have. 

Let's analyse the accomodates attribute first to check for any outliers.
```{r check_accomodates, message = FALSE}
df %>%
  select(accommodates) %>%
  summary()

df %>% 
  ggplot(aes(x=accommodates)) + geom_histogram()
```

From the above ditrbution we can see that number of accomodations becomes scarce after 10 and is skewing the mean. So let us filter out the listings that accomodate more than 10 and then generate a graph for accomodates vs price

```{r AccommodatesVprice, include=TRUE}
df %>%
    filter(accommodates <= 10) %>% #only keep entries with less than or equal to 10 accomodates
    ggplot(aes(x = accommodates, y = price)) + #plot line of best fit
    geom_point() + 
  geom_smooth(method='lm', formula= y~x)
```

Description:
We can see that according to this rough estimate, as expected, the higher amount people that the listing can hold, the higher the price. The regression line has a decent slope to it. 


##### Beds vs. Price
Now, our previous analysis may raise a natural question of checking if the number of beds affects the price. 

Let us check the variation in the number of beds in the data before plotting the required graph.
```{r check_beds, message = FALSE}
df %>% 
  select(beds) %>%
  summary()
#plot
df %>%
  filter(!is.na(beds)) %>%
  ggplot(aes(x=beds)) + geom_histogram()
```

Similar to the previous case, we can see that the data for number of beds becomes scarce after 6 and skews the data. So let us filter out the cases where the number of beds is greater than 6 and then plot the relation


```{r neighbourhoodGroupVprice1, include=TRUE}
df %>% 
    filter(!is.na(beds) & beds <= 6) %>% #filter
    ggplot(aes(x = beds, y = price)) + #plot
    geom_point() + 
  geom_smooth(method='lm', formula= y~x)
```

Description:
Similar to the previous "Accommodates vs. Price" plot, the higher the number of beds at the property the higher the price.

##### Bedrooms vs. Price
We can expect similar results as above when we plot ther elation between the number of bedroom and price in a listing.

Like previous sections, let us check the variation in the data for bedrooms

```{r check_bedrooms, message=FALSE}
df %>% 
  select(bedrooms) %>% 
  summary()
#plot
df %>% 
  filter(!is.na(bedrooms)) %>%
  ggplot(aes(x=bedrooms)) + geom_histogram()
```

The distribution seems okay so we will not have to filter out data in this case.

```{r bedroomsVprice, include=TRUE, message=FALSE}
#calculate mean price for every number of bedrooms
bedroomsVprice <- df %>%
  filter(!is.na(bedrooms)) %>%
    group_by(bedrooms) %>%
    summarize(price_avg = mean(price)) 
#plot line of best fit
df %>%
    filter(!is.na(bedrooms)) %>%
    ggplot(aes(x = bedrooms, y = price)) +
    geom_point() +
  geom_smooth(method='lm', formula= y~x)

```

Description:
As beds, bedrooms and accommodates are closely related, it is not suprising to see that there is also a direct positive relation between bedrooms and price.


##### Bathrooms vs. Price
Once, again we can expect the number of bathrooms in a listing to directly related to the price. Let us first check the variation in the data.

```{r check_num_bathrooms, message = FALSE}
df %>%
  select(bathrooms) %>%
  summary()
#plot
df %>% 
  filter(!is.na(bathrooms)) %>%
  ggplot(aes(x=bathrooms)) +geom_histogram()
```
As seen, 17 bathrooms seems unusual and is not normal. Hence, we should filter out data containing bathroms greater than 5, per the histogram above.

```{r BathroomVprice, include=TRUE, warning = FALSE}
#calculate average price for every number of bedrooms
bathroomsVprice <- df %>%
  filter(!is.na(bathrooms)) %>%
    group_by(bathrooms) %>%
    summarize(price_avg = mean(price))
#plot line of best fit
df %>% 
    filter(bathrooms < 5) %>%
    ggplot(aes(x = bathrooms, y = price)) +
    geom_point() + 
  geom_smooth(method='lm', formula= y~x)
```

Description:
The number of bathrooms is also postively related to the price according to this rough estimate.

Later we can analyse the effect of multiple attributes vs price in the hypothesis teseting section, which will introduce us to the concept of multiple regression.

##### Availability vs. Price

One can expect that if a listing is available for lesser number of days, then either price must be reasonable or its location must be in a busy part of the town. Here let us analyse the relation between price and the availability of a listing in the course of 60 days.

Analysing the availibity attribute, we get
```{r check_availability, message=FALSE}
df %>%
  select(availability_60) %>%
  summary()
#plot
df %>% ggplot(aes(x=availability_60)) + geom_histogram()
```
We see that the availability is extreme on both ends. To avoid skew let us analyse for the range between 3 and 58


```{r AvailabiltyVprice, include=TRUE, warning = FALSE}
#calculate average price by availability
AvailabiltyVprice <- df %>%
  filter(!is.na(availability_60)) %>%
    group_by(availability_60) %>%
    summarize(price_avg = mean(price))
#plot line of best fit on result
AvailabiltyVprice %>%
  filter(availability_60 > 3 & availability_60 < 58) %>%
    ggplot(aes(x = availability_60, y = price_avg)) +
    geom_point() + 
  geom_smooth(method='lm', formula= y~x)
```

Description:
We that there is potentially a postive direct relationship between price and the number of days that the listing is available within the next 60 days. Hence, this relation might not be accurate by itself.

##### Number of Reviews vs. Price

Lets analyse the relation between the number of reviews and price.

```{r check_number_reviews, message = FALSE}
df %>% 
  select(number_of_reviews_ltm) %>%
  summary()
#plot
df %>% 
  ggplot(aes(x=number_of_reviews_ltm)) +
  geom_histogram()
```

Number of listings with no reviews are quite high along with just one of the listings having an unusually large number of reviews. Hence let's filter out all the listings that have the number of reviews to be greater than 100. Moreover, since we need up to date reviews of a listing, we will filter out all the listings that have less then 5 reviews too.

```{r numReviewsVprice, include=TRUE, warning = FALSE}
#plot line of best fit
df %>% 
  filter(!is.na(number_of_reviews_ltm) & number_of_reviews_ltm < 100 & number_of_reviews_ltm > 5) %>%
    ggplot(aes(x = number_of_reviews_ltm, y = price)) +
    geom_point() + 
  geom_smooth(method='lm', formula= y~x)
```

Description:
Contrary to what we expected the number of reviews seems to directly correlate with price negatively.
This might be because the number of reviews might not be an accurate metric as a listing can have many bad reviews and at the same time have a higher price, and vice versa. This means, we should also consider the quality of the listing, which is given by the attribute review_scores_rating 

##### Review Scores vs. Price

Continuing from previous section about the quality of a review, we can analyse the review scores to the price. 

```{r review_scores_check, message = FALSE}
df %>%
  select(review_scores_rating) %>%
  summary()
#plot
df %>%
  filter(!is.na(review_scores_rating)) %>%
  ggplot(aes(x=review_scores_rating)) +
  geom_histogram()
```
We see that all the review scores below 85% tend to be the outliers and hence we need to filter them out before we use
them for our analysis. 
```{r ReviewScoresVprice, include=TRUE, warning = FALSE}
#plot line of best fit
df %>% 
  filter(review_scores_rating> 85 & review_scores_rating<99) %>%
    ggplot(aes(x = review_scores_rating, y = price)) +
    geom_point() + 
  geom_smooth(method='lm', formula= y~x)

```


Description: 
This graphs shows that there is in fact an increasing relationship between price and review scores which fits better with our intuition. However, this metric is still not accurate because the quality of reviews might be good for a various number of reasons like the price of a listing, the nature of the host, the timely communication of the host,etc.


##### Host Listings Count vs. Price

```{r hostListingsVprice, include=TRUE, warning=FALSE}
#plot line of best fit on mean price for host listings count
df %>% 
    group_by(calculated_host_listings_count) %>%
    summarize(price_avg = mean(price)) %>%
    ggplot(aes(x = calculated_host_listings_count, y = price_avg)) +
    geom_point() + 
  geom_smooth(method='lm', formula= y~x)
```

Description:
Although the line is trending upward, examining the data we can see that the data does no support this line very well. From this we determine that calulated_host_listings_count will likely not be an important variable in our model.


### Creating New Attributes

Sometimes, to investigate further, we might need to create new attributes from the existing set of attributes. This part is another important dimension to exploratory data analysis and opens door to ideas for different staistical models. 

#### Does the host have many listings?

Because there was not a noticable relation between the number of listings that the host has and price, we simplify this attribute into a boolean attribute that describes whether the host has "many" listings or not. We do this to differentiate between host who have listings on the side or host that run their listings as a business. We seperate them into two categories. If the host has 1 listing or if the host has multiple listing.
```{r multiListerVprice1, include=TRUE, warning=FALSE}
df <- df %>% # creating a new column that holds a boolean if the listing's host has multiple listings or not
  mutate(manyListings = calculated_host_listings_count > 1) 
df %>%  #displaying table of average price based on if the host has many listings
  group_by(manyListings) %>%
    summarize(price_avg = mean(price))
```

Description:
We can see, from our rough estimate, that hosts with many listings charge lesser than hosts with just one listing.

#### Simplifying Bed Type Attribute
From our earlier analysis of bed_type we can the major difference is whether or not the bed is a real_bed. So, to simplify this categorical attribute we change it to a boolean attribute.
```{r real_bed}
df <- df %>% #creating a new column to determine if the listing has a real bed
  mutate(is_real_bed = bed_type == "Real Bed")

df %>%  #displaying table of average price based on if the listing has a real bed
  group_by(is_real_bed) %>%
    summarize(price_avg = mean(price))
```

## Machine Learning: Linear Regression Model {#machine_learning}
[(Back to index)](#index)

From the previous sections, we picked out several attributes that have an impact on price. Now, we apply machine learning methods to estimate or predict our price. This is one of the last steps in the data science pipeline that gives us the resulting metrics. 

The technique we are about to use is linear regression. Linear regression is a pretty commonly used technique for data analysis. Not only is it used in the EDA, it is also used in machine learning to predict outcomes.

### Creating our model {#c_m}
[(Back to index)](#index)

We proceed by creating a linear regression model use the attributes bathrooms, room_type, is_real_bed, accomadates, manyListings, availabilty_60.The model that we create is a combination of several predctors in an additive form. That is, we will predict the price of listing given a predictor, **holding everything else constant**. Furthermore we will be applying the concept of hypothesis testing. Hypothesis testing is a framwork that organizes what we are trying to predict through framing an initial hypothesis of no relation. For example, in our example, one hypothesis that we can frame is "The price of a listing has no relation to the number of bedrooms". This is called the *null* hypothesis. The goal of our prediction task is to reject this hypothesis of no relation to prove that there is, in fact a significant relation between the two attributes. We used the *p-value* as the metric to reject or accept the null hypothesis. To understand more about how the lm and tidy functions work to create a table click [here](http://varianceexplained.org/r/broom-intro/).

So let us create a linear regression model with all the attributes that we picked from the previous section.

```{r ml1}
#creating a linear regression model 
linear_model <- lm(price~bathrooms+room_type+is_real_bed+accommodates+manyListings+availability_60, data=df)
linear_model_tidy <- linear_model %>% #using tidy to create our table
  tidy()
linear_model_tidy
```

###### Model Analysis - Hypothesis Testing
[(Back to index)](#index)

We can see that the p-value for bathrooms, room_type and accommodates are all below our standard significance level (threshold) of .05. This indicates that these variables have statistical significance from predicting price. Moreover, we can make the following statements as a result:

1. Holding everything else constant, the price of a listing increases by an average of $7 for an additional accommodate
2. Holding everything else constant, the price of a listing increases by an average of $11 for an additional bathroom in the property

#### Alternative Models {#a_m}
[(Back to index)](#index)

We also want to create some other possibly effective models to compare with our original model to determine which model is most accurate. 

##### **Simple Linear Regression Model** {#slrm} 
[(Back to index)](#index)

We create a linear regression model on a fewer number of our seemingly most important attributes: bathrooms, room_type, accommodates, and manyListings. This choice is based on our hypothesis test for the last model. We do this because adding attributes that do not correlate strongly with price can have a negative effect on our model. As a result, it is helpful to compare a complicated model with a simpler model.
```{r ml2}
#creating a second linear regression model 
linear_model_simple <- lm(price~bathrooms+room_type+accommodates+manyListings, data=df)
linear_model_simple_tidy <- linear_model_simple %>% #using tidy to create our table
  tidy()
linear_model_simple_tidy
```

###### Model Analysis - Hypothesis Testing:

Now that we have removed the less significantly important variables, all of our variables' have p-values are less than .05.

##### **Filtered Linear Regression Model** {#flrm} 
[(Back to index)](#index)

To build our second alternatve model we train our model on all listings that accomodate 8 or less people. This is in an effort to not train on outlier data. Training the model on more typical data points, while lowering accuracy to predict outlier data, will increase the precision of predicitng a more typical data point. This is a trade-off that we can examine by comparing our original model to this filtered model to determine if the trade-off increases accuracy in the end.
```{r ml3}
#filtering the dataframe to only include listings that accommodate 8 or less 
filtered_df <- df %>%
  filter(accommodates <= 8)
#creating a third linear regression model 
linear_model_filter <- lm(price~bathrooms+room_type+is_real_bed+accommodates+manyListings+availability_60, data=filtered_df)
linear_model_filter_tidy <- linear_model_filter %>%#using tidy to create our table
  tidy()
linear_model_filter_tidy
```

###### Model Analysis - Hypothesis Testing
Like our original model all variables except is_real_bed and accommodates have p-values less than .05 and are statistically significant.

### Testing our model {#tfdm}
[(Back to index)](#index)

We now create a function for each of our models that produces our models estimation of price for a given data point.

#### Function for Original Model
```{r ml4}
#creating a function to use later that given relevant elements of a listing return the predicted price for our original model
my_estimate_linear <- function(b, r, rb, a, ml, av) {
  #gathering coefficients from model
  b0 <- linear_model_tidy$estimate[1]
  b1 <- linear_model_tidy$estimate[2]
  b2 <- linear_model_tidy$estimate[3]
  b3 <- linear_model_tidy$estimate[4]
  b4 <- linear_model_tidy$estimate[5]
  b5 <- linear_model_tidy$estimate[6]
  b6 <- linear_model_tidy$estimate[7]
  b7 <- linear_model_tidy$estimate[8]
  b8 <- linear_model_tidy$estimate[9]
  #gathering values for variables from parameters
  x_1 <- b
  x_2 <- if (r == "Hotel room") {
    1
  } else {
    0
  }
  x_3 <- ifelse(r == "Private room", 1, 0)
  x_4 <- ifelse(r == "Shared room", 1, 0)
  x_5 <- ifelse(rb, 1, 0)
  x_6 <- a
  x_7 <- ifelse(ml, 1,0)
  x_8 <- av
  #calculating our return value
  y <- b0 + (b1*x_1) + (b2*x_2) + (b3*x_3) + (b4*x_4) + (b5*x_5) + (b6*x_6) + (b7*x_7) + (b8*x_8)
  return (y)
}
```

#### Function for Simple Model
```{r ml5}
#creating a function to use later that given relevant elements of a listing return the predicted price for our simple model
my_estimate_simple <- function(b, r, a, ml) {
  #gathering coefficients from model
  b0 <- linear_model_simple_tidy$estimate[1]
  b1 <- linear_model_simple_tidy$estimate[2]
  b2 <- linear_model_simple_tidy$estimate[3]
  b3 <- linear_model_simple_tidy$estimate[4]
  b4 <- linear_model_simple_tidy$estimate[5]
  b5 <- linear_model_simple_tidy$estimate[6]
  b6 <- linear_model_simple_tidy$estimate[7]
  #gathering values for variables from parameters
  x_1 <- b
  x_2 <- ifelse(r=="Hotel room", 1, 0)
  x_3 <- ifelse(r == "Private room", 1, 0)
  x_4 <- ifelse(r == "Shared room", 1, 0)
  x_5 <- a
  x_6 <- ifelse(ml, 1, 0)
  #calculating our return value
  y <- b0 + (b1*x_1) + (b2*x_2) + (b3*x_3) + (b4*x_4) + (b5*x_5) + (b6*x_6)
  return (y)
}
```

#### Function for Filtered Model
```{r ml6}
#creating a function to use later that given relevant elements of a listing return the predicted price for our filtered model
my_estimate_linear_filter <- function(b, r, rb, a, ml, av) {
  #gathering coefficients from model
  b0 <- linear_model_filter_tidy$estimate[1]
  b1 <- linear_model_filter_tidy$estimate[2]
  b2 <- linear_model_filter_tidy$estimate[3]
  b3 <- linear_model_filter_tidy$estimate[4]
  b4 <- linear_model_filter_tidy$estimate[5]
  b5 <- linear_model_filter_tidy$estimate[6]
  b6 <- linear_model_filter_tidy$estimate[7]
  b7 <- linear_model_filter_tidy$estimate[8]
  b8 <- linear_model_filter_tidy$estimate[9]
  #gathering values for variables from parameters
  x_1 <- b
  x_2 <- if (r == "Hotel room") {
    1
  } else {
    0
  }
  x_3 <- ifelse(r == "Private room", 1, 0)
  x_4 <- ifelse(r == "Shared room", 1, 0)
  x_5 <- ifelse(rb, 1, 0)
  x_6 <- a
  x_7 <- ifelse(ml, 1,0)
  x_8 <- av
  #calculating our return value
  y <- b0 + (b1*x_1) + (b2*x_2) + (b3*x_3) + (b4*x_4) + (b5*x_5) + (b6*x_6) + (b7*x_7) + (b8*x_8)
  return (y)
}
```

### Calulcating Root Mean Squared Error for each model {#ceem}
[(Back to index)](#index)

We then proceed by calculating the residuals squared for each data point, taking the mean of all residuals squared then taking the quare root. For more information on assessing the accuracy of regressions click [here](https://www.theanalysisfactor.com/assessing-the-fit-of-regression-models/).

##### Original Model
``` {r, ml7, message = FALSE}
#create a new row that holds the estimated price using our function on each entry (for our original model)
df$price_estimate <- mapply(my_estimate_linear, df$bathrooms, df$room_type, df$is_real_bed, df$accommodates, df$manyListings, df$availability_60)
#then we compute the residuals for each entry, by subtracting the estimated price from the true price
df <- df %>% 
  mutate(res = (price - price_estimate)^2) %>%
  filter(!is.na(res))
#find the average of the residuals and take the square root to get an idea of the average variance of our estimate from the true price
regular_rss <- sqrt(mean(df$res))
regular_rss

```

##### Simple Model
``` {r, ml8, message = FALSE}
#create a new row that holds the estimated price using our function on each entry (for our simple model)
df$price_estimate <- mapply(my_estimate_simple, df$bathrooms, df$room_type, df$accommodates, df$manyListings)
#then we compute the residuals for each entry, by subtracting the estimated price from the true price
df <- df %>% 
  mutate(res = (price - price_estimate)^2) %>%
  filter(!is.na(res))
#find the average of the residuals and take the square root to get an idea of the average variance of our estimate from the true price
regular_rss <- sqrt(mean(df$res))
regular_rss

```

##### Filter Model
``` {r, ml9, message = FALSE}
#create a new row that holds the estimated price using our function on each entry (for our filtered model)
df$price_estimate <- mapply(my_estimate_linear_filter, df$bathrooms, df$room_type, df$is_real_bed, df$accommodates, df$manyListings, df$availability_60)
#then we compute the residuals for each entry, by subtracting the estimated price from the true price
df <- df %>% 
  mutate(res = (price - price_estimate)^2) %>%
  filter(!is.na(res))
#find the average of the residuals and take the square root to get an idea of the average variance of our estimate from the true price
regular_rss <- sqrt(mean(df$res))
regular_rss

```

## Final Analysis {#fa}
[(Back to index)](#index)

The results above are our Root Mean Square Error (RMSE). The values demonstrate what the average difference is between our price estimate and the actual price of each entry. From these error values, we can see that our original model has the lowest error rate comparitively. However, it does not differ much from the other values. So we would choose our original model to predict price, however from our RMSE values it is likely that choosing our other models would result in approximately equally accurate predictions.
